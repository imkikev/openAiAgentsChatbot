# Multi-Provider AI Agent Chatbot with OpenAI SDK

This project is an AI chatbot powered by the [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/), capable of using different LLM providers, including OpenAI and AWS Bedrock. It features multi-agent orchestration, guardrails, and a Streamlit-based interface for interaction. The project supports secure configuration via `.env` and seamless provider switching using [LiteLLM](https://github.com/BerriAI/litellm).

## ðŸ”§ Powered by OpenAI Agents SDK

This project leverages the [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/), a lightweight and powerful framework for building multi-agent workflows. The SDK simplifies the development of agentic applications by providing:

- **Agents**: LLMs equipped with instructions and tools.
- **Handoffs**: Mechanisms that allow agents to delegate tasks to other agents.
- **Guardrails**: Input validations and checks that run in parallel to agents.
- **Function Tools**: Python functions turned into tools with automatic schema generation and validation.
- **Tracing**: Built-in tracing to visualize and debug agentic flows.

For more details, refer to the [official documentation](https://openai.github.io/openai-agents-python/).

## ðŸš€ Getting Started
---

## Features

- **Chatbot Interface**: A user-friendly chatbot built with Streamlit.
- **Agent Management**: Backend logic for managing multiple agents, including a triage agent and specialized agents.
- **Guardrails**: Input validation and routing using guardrails.
- **Asynchronous Execution**: Supports asynchronous operations for efficient processing.
- **Multi-Provider Support**: Automatically switches between OpenAI and AWS Bedrock (Anthropic Claude 3) using LiteLLM, based on available environment credentials.
- **Testing**: Includes unit tests for backend logic using `pytest` and `pytest-asyncio`.
- **File Retrieval with Vector Store (Optional)**: Retrieve relevant information from your uploaded documents using OpenAIâ€™s `FileSearchTool`.  
  If no `VECTOR_STORE_ID` is provided, the agent will still function normally using only the base model.

## Model Provider Flexibility with LiteLLM

The `sa_genai_agent` is designed to dynamically select between different LLM providers:

- If `AWS_ACCESS_KEY_ID` and related AWS credentials are found in your environment or `.env`, it uses **Anthropic Claude 3 Sonnet** via **AWS Bedrock**.
- If not, it defaults to **OpenAIâ€™s GPT-4o model** â€” no configuration changes needed.

This is enabled using the [LiteLLM](https://github.com/BerriAI/litellm) integration, making the agent flexible and cost-efficient depending on your cloud environment.

### Example `.env`

```env
OPENAI_API_KEY=your_openai_api_key_here
VECTOR_STORE_ID=your_vector_store_id_here
AWS_ACCESS_KEY_ID=your_aws_access_key_id_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key_here
AWS_REGION_NAME=eu-west-1
```
---

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-repo/openAiAgentText.git
   cd openAiAgentText
   ```

2. Install dependencies:
   ```bash
   pip3 install -r requirements.txt
   ```

3. Create a `.env` file in the root directory and add your credentials. The app will use OpenAI by default, but if AWS credentials are present, it will use AWS Bedrock with the Anthropic Claude 3 model:
   ```bash
   OPENAI_API_KEY=your_openai_api_key_here
   VECTOR_STORE_ID=your_vector_store_id_here
   OPENAI_API_KEY=your_openai_api_key_here
   VECTOR_STORE_ID=your_vector_store_id_here (optional)
   AWS_ACCESS_KEY_ID=your_aws_access_key_id_here (optional)
   AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key_here (optional)
   AWS_REGION_NAME=eu-west-1 (optional)
   ```
   - **To obtain a `VECTOR_STORE_ID`** (optional â€“ the app will still work even if you don't provide a vector store):

      - **Upload files via the OpenAI Platform UI:**

         1. Navigate to [OpenAI's platform](https://platform.openai.com/).
         2. Go to the **"Files"** section.      
         3. Create a new vector store and attach your uploaded file.
         4. Retrieve the `VECTOR_STORE_ID` from the vector store details.

      - **Or use the OpenAI CLI:**

         ```bash
         openai file create -p assistants -f my_document.pdf
         openai vector-store create -f <file_id>
         ```

## Usage

### Run the Chatbot
#### To execute the Chatbot logic:
```bash
python3 -m streamlit run frontend/chatbot_app.py
```
#### To execute the backend logic (optional):
```bash
python3 backend/main.py
```

### Run Tests    
To execute the test suite:
```bash
python3 -m pytest tests/test_main.py
```

To save test results to a file:
```bash
python3 -m pytest tests/test_main.py > test_results.txt
```


## Project Structure

```
openAiAgentsChatbot/
â”œâ”€â”€ backend/                    # Backend logic
â”‚   â”œâ”€â”€ agents/                 # Agent definitions
â”‚   â”‚   â”œâ”€â”€ sa_genai_agent.py   # Generative AI specialist agent
â”‚   â”‚   â”œâ”€â”€ sa_operations_agent.py # Operations specialist agent
â”‚   â”œâ”€â”€ prompts/                # YAML files for agent instructions
â”‚   â”‚   â”œâ”€â”€ guardrails.yaml
â”‚   â”‚   â”œâ”€â”€ sa_operations_instructions.yaml
â”‚   â”‚   â””â”€â”€ sa_genai_agent_instructions.yaml
â”‚   â”œâ”€â”€ [main.py](http://_vscodecontentref_/3)                 # Backend entry point
â”œâ”€â”€ frontend/                   # Frontend logic
â”‚   â”œâ”€â”€ [chatbot_app.py](http://_vscodecontentref_/4)          # Streamlit chatbot application
â”œâ”€â”€ tests/                      # Unit and integration tests
â”‚   â”œâ”€â”€ test_main.py            # Tests for backend logic
â”œâ”€â”€ .env                        # Environment variables (ignored by Git)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ [README.md](http://_vscodecontentref_/5)                   # Project documentation
```

## Dependencies

Install the following Python packages:

- `openai-agents`
- `python-dotenv`
- `pyyaml`
- `openai`
- `streamlit`
- `pytest`
- `pytest-asyncio`

You can install all dependencies using:
```bash
pip3 install -r requirements.txt
```
or one by one:
```bash
pip3 install openai-agents  
pip3 install python-dotenv
pip3 install pyyaml    
pip3 install openai streamlit
pip3 install pytest
pip3 install pytest-asyncio
pip3 install "openai-agents[litellm]"
pip3 install litellm
```

## Author

Developed by Kike.

---

### Key Improvements
1. **Detailed Usage Instructions**: Includes commands for running the backend, chatbot, and tests.
2. **Project Structure**: Provides an overview of the folder structure for better understanding.
3. **Environment Variables**: Explains how to configure the [.env](http://_vscodecontentref_/9) file.
4. **Testing**: Includes commands for running tests and generating reports.
5. **Troubleshooting**: Addresses common issues like missing environment variables or dependencies.

Let me know if you'd like to customize this further!

---

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.
